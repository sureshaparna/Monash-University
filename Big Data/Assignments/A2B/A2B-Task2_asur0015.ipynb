{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Predicting sales data using Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing the libraries and statements\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "#creating spark configuration object\n",
    "processing_cores = \"local[4]\"\n",
    "\n",
    "#application name\n",
    "application_name = \"assignment2b\"\n",
    "\n",
    "configuration = SparkConf().setMaster(processing_cores).setAppName(application_name)\n",
    "\n",
    "#creating the spark session\n",
    "spark = SparkSession.builder.config(conf = configuration).getOrCreate()\n",
    "\n",
    "# creating spark context and setting a checkpoiint directory\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sc.setCheckpointDir(\"spark_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define schema and load file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#creating a schema for the stores data\n",
    "stores_schema = StructType() \\\n",
    ".add(\"Store\", IntegerType(), True) \\\n",
    ".add(\"Type\", StringType(), True) \\\n",
    ".add(\"Size\", IntegerType(), True)\n",
    "\n",
    "#reading the stores data\n",
    "stores_df = spark.read.format(\"csv\").option(\"header\", True).options(delimiter = ',').schema(stores_schema).load(\"stores.csv\")\n",
    "stores_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Injest Kafka data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "hostip = \"192.168.62.158\"\n",
    "\n",
    "#reading the encoded data from the kafka producer stream\n",
    "spark_consumer_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", f'{hostip}:9092').option(\"encoding\", \"UTF-8\") \\\n",
    ".option(\"subscribe\", \"kafka_producer_stream\").option(\"startingOffsets\", \"latest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Temperature: string (nullable = true)\n",
      " |-- Fuel_Price: string (nullable = true)\n",
      " |-- MarkDown1: string (nullable = true)\n",
      " |-- MarkDown2: string (nullable = true)\n",
      " |-- MarkDown3: string (nullable = true)\n",
      " |-- MarkDown4: string (nullable = true)\n",
      " |-- MarkDown5: string (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      " |-- IsHoliday: string (nullable = true)\n",
      " |-- last_weekly_sales: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating the schema for the data from the kafka stream\n",
    "#ingesting \"ts\" as Long type and others as string type\n",
    "producer_schema = ArrayType(StructType() \\\n",
    ".add(\"Store\", StringType(), True) \\\n",
    ".add(\"Date\", StringType(), True) \\\n",
    ".add(\"Temperature\", StringType(), True) \\\n",
    ".add(\"Fuel_Price\", StringType(), True) \\\n",
    ".add(\"MarkDown1\", StringType(), True) \\\n",
    ".add(\"MarkDown2\", StringType(), True) \\\n",
    ".add(\"MarkDown3\", StringType(), True) \\\n",
    ".add(\"MarkDown4\", StringType(), True) \\\n",
    ".add(\"MarkDown5\", StringType(), True) \\\n",
    ".add(\"CPI\", StringType(), True) \\\n",
    ".add(\"Unemployment\", StringType(), True) \\\n",
    ".add(\"IsHoliday\", StringType(), True) \\\n",
    ".add(\"last_weekly_sales\", StringType(), True) \\\n",
    ".add(\"ts\", LongType(), True))\n",
    "\n",
    "#converting the values from the kafka data stream to string\n",
    "df = spark_consumer_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# parsing the string to the json format based on the defined schema\n",
    "df = df.select(from_json(col(\"value\"), producer_schema).alias('parsed_data'))\n",
    "\n",
    "# exploding the parsed json and getting the nested values\n",
    "df = df.select(explode(col(\"parsed_data\")).alias(\"data\"))\n",
    "\n",
    "# \".*\" for selecting the nested values\n",
    "df = df.selectExpr(\"data.*\")\n",
    "\n",
    "#printing the schema for the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Persist raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#persisting the exploded data and writing the stream in parquet format\n",
    "#and saving the parquet file in the given location\n",
    "persisted_sink = df.writeStream \\\n",
    ".outputMode(\"append\").format(\"parquet\") \\\n",
    ".option(\"path\", \"persisted_parquet_data\") \\\n",
    ".option(\"checkpointLocation\", \"persisted_parquet_data/checkpoint\") \\\n",
    ".trigger(processingTime = \"5 seconds\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|Store|Date      |Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|CPI      |Unemployment|IsHoliday|last_weekly_sales |ts        |\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|1    |2011-02-04|42.27      |2.989     |nan      |nan      |nan      |nan      |nan      |212.56688|7.742       |false    |1316899.3076019287|1675857813|\n",
      "|10   |2011-02-04|44.88      |3.348     |nan      |nan      |nan      |nan      |nan      |127.71958|8.744       |false    |1715769.0571422577|1675857813|\n",
      "|11   |2011-02-04|47.17      |2.989     |nan      |nan      |nan      |nan      |nan      |215.88634|7.551       |false    |1100418.6917500496|1675857813|\n",
      "|12   |2011-02-04|45.14      |3.348     |nan      |nan      |nan      |nan      |nan      |127.71958|14.021      |false    |873119.0579872131 |1675857813|\n",
      "|13   |2011-02-04|23.35      |2.974     |nan      |nan      |nan      |nan      |nan      |127.71958|7.47        |false    |1633663.1241288185|1675857813|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading the parquet result and showing the file\n",
    "parquet_df = spark.read.parquet(\"persisted_parquet_data/*.snappy.parquet\")\n",
    "parquet_df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted_sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Transform data formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: double (nullable = true)\n",
      " |-- MarkDown2: double (nullable = true)\n",
      " |-- MarkDown3: double (nullable = true)\n",
      " |-- MarkDown4: double (nullable = true)\n",
      " |-- MarkDown5: double (nullable = true)\n",
      " |-- CPI: double (nullable = true)\n",
      " |-- Unemployment: double (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- last_weekly_sales: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a new dataframe - \"formatted_df\" by transforming the datatypes in the dataframe - \"df\" \n",
    "formatted_df = df.withColumn(\"Store\", df[\"Store\"].cast(IntegerType())) \\\n",
    ".withColumn(\"Date\", df[\"Date\"].cast(DateType())) \\\n",
    ".withColumn(\"Temperature\", df[\"Temperature\"].cast(DoubleType())) \\\n",
    ".withColumn(\"Fuel_Price\", df[\"Fuel_Price\"].cast(DoubleType())) \\\n",
    ".withColumn(\"MarkDown1\", df[\"MarkDown1\"].cast(DoubleType())) \\\n",
    ".withColumn(\"MarkDown2\", df[\"MarkDown2\"].cast(DoubleType())) \\\n",
    ".withColumn(\"MarkDown3\", df[\"MarkDown3\"].cast(DoubleType())) \\\n",
    ".withColumn(\"MarkDown4\", df[\"MarkDown4\"].cast(DoubleType())) \\\n",
    ".withColumn(\"MarkDown5\", df[\"MarkDown5\"].cast(DoubleType())) \\\n",
    ".withColumn(\"CPI\", df[\"CPI\"].cast(DoubleType())) \\\n",
    ".withColumn(\"Unemployment\", df[\"Unemployment\"].cast(DoubleType())) \\\n",
    ".withColumn(\"IsHoliday\", df[\"IsHoliday\"].cast(BooleanType())) \\\n",
    ".withColumn(\"last_weekly_sales\", df[\"last_weekly_sales\"].cast(DoubleType())) \\\n",
    ".withColumn(\"ts\", df[\"ts\"].cast(TimestampType()))\n",
    "\n",
    "#printing the schema for the data\n",
    "formatted_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Prepare feature columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: double (nullable = true)\n",
      " |-- MarkDown2: double (nullable = true)\n",
      " |-- MarkDown3: double (nullable = true)\n",
      " |-- MarkDown4: double (nullable = true)\n",
      " |-- MarkDown5: double (nullable = true)\n",
      " |-- CPI: double (nullable = true)\n",
      " |-- Unemployment: double (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- last_weekly_sales: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_year: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#creating new columns - month, day of the month, day of the year and week of the year from the date column\n",
    "formatted_df = formatted_df.withColumn(\"Month\", F.month(\"Date\"))\n",
    "formatted_df = formatted_df.withColumn(\"day_of_month\", F.dayofmonth(\"Date\"))\n",
    "formatted_df = formatted_df.withColumn(\"day_of_year\", F.dayofyear(\"Date\"))\n",
    "formatted_df = formatted_df.withColumn(\"week_of_year\", F.weekofyear(\"Date\"))\n",
    "\n",
    "formatted_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Join the local data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: double (nullable = true)\n",
      " |-- MarkDown2: double (nullable = true)\n",
      " |-- MarkDown3: double (nullable = true)\n",
      " |-- MarkDown4: double (nullable = true)\n",
      " |-- MarkDown5: double (nullable = true)\n",
      " |-- CPI: double (nullable = true)\n",
      " |-- Unemployment: double (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- last_weekly_sales: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_year: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#joining the formatted_df with stores df based on store id \n",
    "joined_df = formatted_df.join(stores_df, [\"Store\"], \"inner\")\n",
    "\n",
    "#printing the schema for the joined dataframe\n",
    "joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Perform predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "#loading the ml model provided \n",
    "model = PipelineModel.load(\"sales_estimation_pipeline_model\")\n",
    "\n",
    "#transforming the joined dataframe\n",
    "predictions_df = model.transform(joined_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writing the stream of predictions into parquet files\n",
    "predictions_sink = predictions_df.writeStream \\\n",
    ".outputMode(\"append\").format(\"parquet\") \\\n",
    ".option(\"path\", \"predictions_parquet_data\") \\\n",
    ".option(\"checkpointLocation\", \"predictions_parquet_data/checkpoint\") \\\n",
    ".trigger(processingTime = \"5 seconds\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+-------------------+-----+------------+-----------+------------+----+------+--------+-------------+------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|Store|Date      |Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|CPI      |Unemployment|IsHoliday|last_weekly_sales |ts                 |Month|day_of_month|day_of_year|week_of_year|Type|Size  |Type_idx|Type_vec     |features                                                                                                          |prediction       |\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+-------------------+-----+------------+-----------+------------+----+------+--------+-------------+------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|1    |2011-02-25|62.9       |3.065     |NaN      |NaN      |NaN      |NaN      |NaN      |213.53561|7.742       |false    |1686842.7817020416|2023-02-08 12:03:48|2    |25          |56         |8           |A   |151315|0.0     |(3,[0],[1.0])|[1.0,62.9,3.065,NaN,NaN,NaN,NaN,NaN,213.53561,7.742,0.0,151315.0,2.0,25.0,56.0,8.0,1686842.7817020416,1.0,0.0,0.0]|1567502.877105667|\n",
      "|10   |2011-02-25|53.59      |3.398     |NaN      |NaN      |NaN      |NaN      |NaN      |128.13   |8.744       |false    |2106934.5675640106|2023-02-08 12:03:48|2    |25          |56         |8           |B   |126512|1.0     |(3,[1],[1.0])|[10.0,53.59,3.398,NaN,NaN,NaN,NaN,NaN,128.13,8.744,0.0,126512.0,2.0,25.0,56.0,8.0,2106934.5675640106,0.0,1.0,0.0] |2010311.948944334|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+-------------------+-----+------------+-----------+------------+----+------+--------+-------------+------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading the predictions parquet file and showing the data\n",
    "predictions = spark.read.parquet(\"predictions_parquet_data/*.snappy.parquet\")\n",
    "predictions.show(2, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping the streaming of predictions\n",
    "predictions_sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 write code to process the data following requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering stores that have (weekly_sales prediction / store size) > 8.5 in the predictions dataframe\n",
    "#abandoning the data received after 3 seconds using watermark\n",
    "#showing the grouped data with window duration 10 seconds and slide duration 5 seconds\n",
    "\n",
    "result_df = predictions_df.filter((predictions_df.prediction)/(predictions_df.Size) > 8.5) \\\n",
    ".withWatermark(\"ts\", \"3 seconds\") \\\n",
    ".groupBy(F.window(\"ts\", windowDuration = \"10 seconds\", slideDuration = \"5 seconds\"), \"Type\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the data where the number of rows >0\n",
    "# i.e. showing the count where there is data\n",
    "\n",
    "def print_function(df, epoch_id):\n",
    "    if(df.count() > 0):\n",
    "        df.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----+-----+\n",
      "|window                                    |Type|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2023-02-08 12:04:55, 2023-02-08 12:05:05}|C   |5    |\n",
      "|{2023-02-08 12:04:55, 2023-02-08 12:05:05}|A   |9    |\n",
      "|{2023-02-08 12:05:00, 2023-02-08 12:05:10}|C   |5    |\n",
      "+------------------------------------------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#writing the result stream into a memory sink  \n",
    "#and processing each batch of data every 5 seconds\n",
    "result_sink = result_df.writeStream.outputMode(\"complete\")\\\n",
    "            .format(\"memory\").queryName(\"data\") \\\n",
    "            .foreachBatch(print_function)\\\n",
    "            .trigger(processingTime = '5 seconds')\\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping the streaming of results \n",
    "result_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 average weekly sales predictions of different types of stores and write the stream back to Kafka sink using a different topic name\n",
    "\n",
    "The data you sended should be like this:\n",
    "\n",
    "|  key   | value  |\n",
    "|  ----  | ----  |\n",
    "| timestamp of window start | JSON of store type and avg sales |\n",
    "| '1673233646'  | '{\"Type\":\"A\",\"predict_weekly_sales\":20000}' |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe that shows the average predictions from the predictions_df\n",
    "#abandoning the data received after 3 seconds using watermark\n",
    "#showing the grouped data with window duration 10 seconds and slide duration 5 seconds\n",
    "\n",
    "stream_df = predictions_df.withWatermark(\"ts\", \"3 seconds\") \\\n",
    ".groupBy(F.window(\"ts\", windowDuration = \"10 seconds\", slideDuration = \"5 seconds\"), \"Type\") \\\n",
    ".agg(avg(\"prediction\").alias(\"predict_weekly_sales\"))\n",
    "\n",
    "#creating a json format column called value that holds the type and predicted weekly sales\n",
    "#selecting the start values from window function and converting it into unix timestamp \n",
    "#and naming the column as \"key\"\n",
    "kafka_df = stream_df.withColumn(\"value\", to_json(struct(\"Type\", \"predict_weekly_sales\"))) \\\n",
    "            .withColumn(\"key\", F.unix_timestamp(stream_df.window.start)) \\\n",
    "            .select(\"key\", \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the stream back to Kafka sink using a different topic name - pyspark stream\n",
    "kafka_sink = kafka_df.selectExpr(\"CAST(key AS STRING) AS key\", \"CAST(value AS STRING) AS value\") \\\n",
    "            .writeStream.format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\") \\\n",
    "            .option(\"topic\", \"pyspark_stream\") \\\n",
    "            .option(\"startingOffsets\", \"latest\") \\\n",
    "            .option(\"checkpointLocation\", \"pyspark_stream_checkpoint\") \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping the kafka stream sink\n",
    "kafka_sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### For cleaning up the quries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for i in [\"persisted_parquet_data\", \"predictions_parquet_data\", \"pyspark_stream_checkpoint\", \"spark_checkpoint\"]:\n",
    "    try:\n",
    "        shutil.rmtree(i)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Tutorials week- 9,10,11\n",
    "\n",
    "- afsarafsar                    6911 silver badge99 bronze badges (1964) How to convert spark streaming nested JSON coming on Kafka to flat dataframe?, Stack Overflow. Available at: https://stackoverflow.com/questions/46204750/how-to-convert-spark-streaming-nested-json-coming-on-kafka-to-flat-dataframe (Accessed: February 8, 2023). \n",
    "\n",
    "- Kukreja, M. (2020) Track real-time gold prices using Apache Kafka, Pandas &amp; matplotlib, Medium. Towards Data Science. Available at: https://towardsdatascience.com/track-real-time-gold-prices-using-apache-kafka-pandas-matplotlib-122a73728a88 (Accessed: February 8, 2023). \n",
    "\n",
    "- Structured Streaming + kafka integration guide (kafka broker version 0.10.0 or higher) (no date) Structured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher) - Spark 3.3.1 Documentation. Available at: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html (Accessed: February 8, 2023). \n",
    "\n",
    "- Structured Streaming Programming Guide (no date) Structured Streaming Programming Guide - Spark 3.3.1 Documentation. Available at: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html (Accessed: February 8, 2023). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
